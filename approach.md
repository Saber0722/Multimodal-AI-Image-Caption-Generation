# ğŸ–¼ï¸ Multimodal AI: Image Captioning with Vision Encoder + Transformer Decoder

## ğŸ“Œ Project Overview

This project builds a **multimodal encoderâ€“decoder architecture** that generates natural language captions from images.

The system consists of:

* **Vision Encoder (CNN or ViT)** â†’ Extracts image features
* **Transformer Decoder** â†’ Generates text captions autoregressively
* **Evaluation Pipeline** â†’ BLEU, ROUGE metrics
* **Decoding Comparison** â†’ Greedy vs Beam Search

This is a modern adaptation of the classic *Show and Tell* model, upgraded with Transformer-based decoding.

---

# ğŸ§  High-Level Architecture

```
Image â†’ Vision Encoder â†’ Feature Embeddings â†’ Transformer Decoder â†’ Caption
```

Detailed flow:

```
Input Image
      â†“
Pretrained CNN / ViT
      â†“
Image Feature Embeddings
      â†“
Projection Layer (to decoder dimension)
      â†“
Transformer Decoder (masked self-attention + cross-attention)
      â†“
Linear + Softmax
      â†“
Generated Caption
```

---

# ğŸ— System Architecture Components

---

## 1ï¸âƒ£ Dataset


### Option A: COCO Mini

![Image](https://www.researchgate.net/publication/376235886/figure/fig6/AS%3A11431281241523148%401715134617560/Sample-images-from-the-COCO-dataset-and-captions-predicted-by-the-Base-version-of-our.png)

![Image](https://cocodataset.org/images/coco-examples.jpg)

![Image](https://www.labellerr.com/blog/content/images/2023/06/Screenshot-2023-05-31-234904.png)

![Image](https://cdn.labellerr.com/COCO/Screenshot%202023-06-01%20100603.webp)

* Subset of MS COCO
* Rich object diversity
* Better generalization

### Option B: Coco Large
---

## 2ï¸âƒ£ Vision Encoder (Image Feature Extractor)

Two strong choices:

---

Below is the **modified documentation section**, aligned with your final three experimental setups:

---

# ğŸ”¹ Option A: ResNet-50 Encoder (COCO-mini) + GPT Decoder

![Image](https://www.researchgate.net/publication/356162462/figure/fig3/AS%3A1089285335846971%401636717270496/The-architecture-of-the-ResNet-50-network.jpg)

![Image](https://www.researchgate.net/publication/350524328/figure/fig1/AS%3A1007436949364737%401617203094867/Resnet-Architectures-Right-And-Residual-Block-Top-Left-Bottleneck-Layer-Bottom.ppm)

![Image](https://www.researchgate.net/publication/376497473/figure/fig2/AS%3A11431281212236571%401702573294749/An-example-for-feature-map-visualizations-in-ResNet50-The-input-images-contain-NIR-Red.png)

![Image](https://www.researchgate.net/publication/337538111/figure/fig5/AS%3A901560490012674%401591960179448/sualization-of-the-feature-maps-in-the-ResNet-50-and-VGG-16-models-trained-by-the-two.jpg)

### ğŸ“Œ Configuration

* **Encoder:** Microsoft Research ResNet-50
* **Dataset:** COCO-mini
* **Decoder:** GPT-style Transformer decoder

---

### ğŸ§  Architecture

**Encoder Modifications**

* Remove final classification layer
* Extract:

  * Global pooled feature (2048-d), or
  * Spatial feature map (7Ã—7Ã—2048)

**Recommended for captioning:**

* Use spatial features for cross-attention

---

### ğŸ“¦ Output Formats

| Type             | Shape         | Usage                         |
| ---------------- | ------------- | ----------------------------- |
| Global vector    | (B, 2048)     | Simple prefix conditioning    |
| Spatial features | (B, 49, 2048) | Cross-attention (recommended) |

---

### ğŸ¯ Why This Setup?

âœ” Stable baseline
âœ” Lightweight (fits 8GB easily)
âœ” Works well for medium-small dataset
âœ” Clean encoderâ€“decoder separation

This serves as your **controlled baseline experiment**.

---

# ğŸ”¹ Option B: CLIP Vision Encoder (COCO-Large ~80k Images)

![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2Ah5xJzfFAfjdysNvqQbB9nQ.png)

![Image](https://miro.medium.com/1%2AuVUI6bU49oT-nNRGFs4GEA.png)

![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2Al37va2Mu8Snx6LLb13430A.png)

![Image](https://substackcdn.com/image/fetch/%24s_%21R7V_%21%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F998aad2b-fb98-458c-9cbb-4ba036e32e60_800x565.png)

### ğŸ“Œ Configuration

* **Encoder:** CLIP Vision Encoder (ViT-B/16)
* Developed by OpenAI
* **Dataset:** COCO Large (~80k images)
* **Decoder:** GPT-style decoder

---

### ğŸ§  How CLIP Helps

CLIP is pretrained on **400M imageâ€“text pairs** using contrastive learning.

Instead of classification, it learns:

> Image â†” Text semantic alignment

This makes CLIP embeddings:

* Language-aware
* Context-aware
* Better suited for caption generation

---

### ğŸ“¦ Output

* Patch embeddings: (B, N, 768)
* Already aligned to text semantic space

---

### ğŸ¯ Why This Setup?

âœ” Strongest semantic understanding
âœ” Best expected BLEU/ROUGE
âœ” Ideal for large dataset (80k images)
âœ” Reduces learning burden on decoder

This is your **high-performance configuration**.

---

# ğŸ”¹ Option C: Vision Transformer (ViT-B/16) + GPT Decoder (COCO-Large)

![Image](https://blog.roboflow.com/content/images/2025/04/Screenshot-2025-04-17-at-1.30.34-PM.png)

![Image](https://cdn.sanity.io/images/vr8gru94/production/7a096efc8f3cc40849ee17a546dc0e685da2dc73-4237x1515.png)

![Image](https://theaisummer.com/static/aa65d942973255da238052d8cdfa4fcd/7d4ec/the-transformer-block-vit.png)

![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2Al37va2Mu8Snx6LLb13430A.png)

### ğŸ“Œ Configuration

* **Encoder:** ViT-B/16
* Developed by Google Research
* **Dataset:** COCO Large (~80k images)
* **Decoder:** GPT-style Transformer decoder

---

### ğŸ§  How It Works

* Split image into 16Ã—16 patches
* Linear projection â†’ patch embeddings
* Add positional encodings
* Process through transformer encoder

Output:

* Patch embeddings (B, N, 768)

---

### ğŸ¯ Why This Setup?

âœ” Pure transformer pipeline
âœ” Global self-attention
âœ” Cleaner theoretical alignment with GPT decoder
âœ” Good scaling with larger datasets

This serves as your **pure transformer experiment**.

---

# 3ï¸âƒ£ Feature Projection Layer (All Setups)

Since encoder output dimension â‰  decoder dimension:

```
Image Features â†’ Linear Layer â†’ d_model (e.g., 512 or 768)
```

### Purpose:

* Align encoder embeddings with GPT decoder input dimension
* Enable cross-attention compatibility

Example:

* ResNet output: 2048 â†’ 512
* ViT/CLIP output: 768 â†’ 512

---

# 4ï¸âƒ£ GPT-Style Transformer Decoder (Text Generator)

![Image](https://miro.medium.com/0%2A376uJu_fc_uR8H3X.png)

![Image](https://cdn.sanity.io/images/jo7n4k8s/production/25ebbba9d2ce12efc8c3da181942367f05c795be-2386x1338.jpg?auto=format)

![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2AxzvpKDgLm2A-D9C04V4rOw.png)

![Image](https://miro.medium.com/v2/resize%3Afit%3A2000/1%2AF4EZBBYoQN3pAqk8tXC8sA.png)

---

## ğŸ§  Decoder Architecture

Each decoder layer contains:

### 1ï¸âƒ£ Masked Self-Attention

* Prevents access to future tokens
* Autoregressive generation

### 2ï¸âƒ£ Cross-Attention

* Queries = text tokens
* Keys/Values = image embeddings

### 3ï¸âƒ£ Feed Forward Network

* Position-wise MLP
* Non-linear transformation

---

## ğŸ“¥ Input to Decoder

* `<SOS>` token
* Previously generated tokens
* Positional encoding

---

## ğŸ“¤ Output Layer

```
Decoder Output â†’ Linear Layer â†’ Vocabulary Size â†’ Softmax
```

Produces probability distribution over next word.

---

# ğŸ”¬ Summary of Your Three Experiments

| Setup           | Dataset    | Strength                | Purpose                 |
| --------------- | ---------- | ----------------------- | ----------------------- |
| ResNet-50 + GPT | COCO-mini  | Stable baseline         | Controlled experiment   |
| CLIP + GPT      | COCO-large | Best semantic alignment | Highest performance     |
| ViT + GPT       | COCO-large | Pure transformer        | Architecture comparison |

---

If you'd like, I can now:

* Add expected BLEU/ROUGE per setup
* Provide architectural diagrams combining encoder + GPT
* Add training strategy per configuration
* Help you write the experimental comparison section for your report


# ğŸ” Training Pipeline

### 1ï¸âƒ£ Preprocessing

* Resize image (224Ã—224)
* Normalize (ImageNet stats)
* Tokenize captions
* Pad sequences
* Add `<SOS>`, `<EOS>`

---

### 2ï¸âƒ£ Training Objective

**Cross-Entropy Loss**

```
Loss = - Î£ log P(target_word | previous_words, image)
```

Teacher forcing used during training.

---

### 3ï¸âƒ£ Optimization

* Adam optimizer
* Learning rate scheduler (optional)
* Freeze encoder initially (recommended for small datasets)

---

# ğŸ” Inference & Decoding Strategies

---

## ğŸ”¹ Greedy Decoding

```
At each step:
    Select word with highest probability
```

âœ” Fast
âœ– Can miss better global sequence

---

## ğŸ”¹ Beam Search

```
Keep top-k candidate sequences at each step
```

Example:

Beam size = 3
Keep 3 best partial sentences at every timestep.

âœ” Better captions
âœ– Slower

---

### Expected Comparison

| Metric | Greedy   | Beam Search |
| ------ | -------- | ----------- |
| BLEU   | Moderate | Higher      |
| ROUGE  | Moderate | Higher      |
| Speed  | Fast     | Slower      |

---

# ğŸ“Š Evaluation Metrics

---

## ğŸ”¹ BLEU (Bilingual Evaluation Understudy)

Measures n-gram precision overlap.

BLEU-1 â†’ Unigram
BLEU-4 â†’ 4-gram

Higher = better

---

## ğŸ”¹ ROUGE

Measures recall overlap.

ROUGE-L â†’ Longest Common Subsequence

---

## Optional: CIDEr (if dataset supports)

Better for image captioning tasks.

---

# ğŸ“¦ Full Model Architecture Diagram (Conceptual)

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚      Input Image     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   CNN / ViT Encoder  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Feature Projection â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚        Transformer Decoder      â”‚
         â”‚  Masked Self-Attention          â”‚
         â”‚  Cross Attention (Image)        â”‚
         â”‚  Feed Forward                   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Linear + Softmax    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
               Generated Caption
```

---
