{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53ce7736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saber/Wox/ANLP/multimodal_ai/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Using device:</span> cuda\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mUsing device:\u001b[0m cuda\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from rich.console import Console\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "console = Console()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "console.print(f\"[bold green]Using device:[/bold green] {device}\")\n",
    "\n",
    "PROJECT_ROOT = Path(\"../\").resolve()\n",
    "\n",
    "VAL_CSV = PROJECT_ROOT / \"data\" / \"processed\" / \"coco_val_20k.csv\"\n",
    "CHECKPOINT_PATH = PROJECT_ROOT / \"checkpoints\" / \"best_model.pt\"\n",
    "\n",
    "assert VAL_CSV.exists(), \"Validation CSV not found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8c05661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(VAL_CSV)\n",
    "\n",
    "# For faster experimentation, evaluate on 2000 samples first\n",
    "df = df.sample(2000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bfe6767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def compute_embeddings(model, processor, dataframe):\n",
    "    model.eval()\n",
    "    image_embeddings = []\n",
    "    text_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(dataframe))):\n",
    "            row = dataframe.iloc[i]\n",
    "            image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "            caption = row[\"caption\"]\n",
    "\n",
    "            inputs = processor(\n",
    "                text=[caption],\n",
    "                images=[image],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            ).to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            image_embeds = outputs.image_embeds\n",
    "            text_embeds = outputs.text_embeds\n",
    "\n",
    "            image_embeddings.append(image_embeds.cpu())\n",
    "            text_embeddings.append(text_embeds.cpu())\n",
    "\n",
    "    image_embeddings = torch.cat(image_embeddings)\n",
    "    text_embeddings = torch.cat(text_embeddings)\n",
    "\n",
    "    # Normalize\n",
    "    image_embeddings /= image_embeddings.norm(dim=1, keepdim=True)\n",
    "    text_embeddings /= text_embeddings.norm(dim=1, keepdim=True)\n",
    "\n",
    "    return image_embeddings, text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c8fe4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval metrics\n",
    "\n",
    "def recall_at_k(similarity, k):\n",
    "    correct = 0\n",
    "    for i in range(len(similarity)):\n",
    "        if i in similarity[i].topk(k).indices:\n",
    "            correct += 1\n",
    "    return correct / len(similarity)\n",
    "\n",
    "\n",
    "def mean_reciprocal_rank(similarity):\n",
    "    reciprocal_ranks = []\n",
    "    for i in range(len(similarity)):\n",
    "        sorted_indices = similarity[i].argsort(descending=True)\n",
    "        rank = (sorted_indices == i).nonzero(as_tuple=True)[0].item() + 1\n",
    "        reciprocal_ranks.append(1 / rank)\n",
    "    return np.mean(reciprocal_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7af29dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Evaluating PRETRAINED CLIP</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mEvaluating PRETRAINED CLIP\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 398/398 [00:00<00:00, 1694.54it/s, Materializing param=visual_projection.weight]                                \n",
      "\u001b[1mCLIPModel LOAD REPORT\u001b[0m from: openai/clip-vit-base-patch32\n",
      "Key                                  | Status     |  | \n",
      "-------------------------------------+------------+--+-\n",
      "text_model.embeddings.position_ids   | UNEXPECTED |  | \n",
      "vision_model.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "The image processor of type `CLIPImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n",
      "100%|██████████| 2000/2000 [00:20<00:00, 95.36it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Recall@<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4030</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Recall@\u001b[1;36m1\u001b[0m: \u001b[1;36m0.4030\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Recall@<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7345</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Recall@\u001b[1;36m5\u001b[0m: \u001b[1;36m0.7345\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Recall@<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8325</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Recall@\u001b[1;36m10\u001b[0m: \u001b[1;36m0.8325\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">MRR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5523</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "MRR: \u001b[1;36m0.5523\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate pretrained CLIP\n",
    "\n",
    "console.print(\"[bold cyan]Evaluating PRETRAINED CLIP[/bold cyan]\")\n",
    "\n",
    "pretrained_model = CLIPModel.from_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\",\n",
    "    use_safetensors=True\n",
    ").to(device)\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "img_emb, txt_emb = compute_embeddings(pretrained_model, processor, df)\n",
    "\n",
    "similarity = img_emb @ txt_emb.T\n",
    "\n",
    "for k in [1,5,10]:\n",
    "    console.print(f\"Recall@{k}: {recall_at_k(similarity, k):.4f}\")\n",
    "\n",
    "console.print(f\"MRR: {mean_reciprocal_rank(similarity):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51542016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Evaluating FINE-TUNED CLIP (LoRA)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;33mEvaluating FINE-TUNED CLIP \u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mLoRA\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 398/398 [00:00<00:00, 1869.60it/s, Materializing param=visual_projection.weight]                                \n",
      "\u001b[1mCLIPModel LOAD REPORT\u001b[0m from: openai/clip-vit-base-patch32\n",
      "Key                                  | Status     |  | \n",
      "-------------------------------------+------------+--+-\n",
      "text_model.embeddings.position_ids   | UNEXPECTED |  | \n",
      "vision_model.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "100%|██████████| 2000/2000 [00:21<00:00, 93.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Recall@<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4705</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Recall@\u001b[1;36m1\u001b[0m: \u001b[1;36m0.4705\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Recall@<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8160</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Recall@\u001b[1;36m5\u001b[0m: \u001b[1;36m0.8160\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Recall@<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8985</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Recall@\u001b[1;36m10\u001b[0m: \u001b[1;36m0.8985\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">MRR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6225</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "MRR: \u001b[1;36m0.6225\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "console.print(\"\\n[bold yellow]Evaluating FINE-TUNED CLIP (LoRA)[/bold yellow]\")\n",
    "\n",
    "# Load base model\n",
    "finetuned_model = CLIPModel.from_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\",\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "# Apply SAME LoRA config as training\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "finetuned_model.text_model = get_peft_model(\n",
    "    finetuned_model.text_model,\n",
    "    peft_config\n",
    ")\n",
    "\n",
    "# Load weights\n",
    "state_dict = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "finetuned_model.load_state_dict(state_dict)\n",
    "\n",
    "finetuned_model.to(device)\n",
    "finetuned_model.eval()\n",
    "\n",
    "# Compute embeddings\n",
    "img_emb_ft, txt_emb_ft = compute_embeddings(\n",
    "    finetuned_model,\n",
    "    processor,\n",
    "    df\n",
    ")\n",
    "\n",
    "similarity_ft = img_emb_ft @ txt_emb_ft.T\n",
    "\n",
    "for k in [1,5,10]:\n",
    "    console.print(f\"Recall@{k}: {recall_at_k(similarity_ft, k):.4f}\")\n",
    "\n",
    "console.print(f\"MRR: {mean_reciprocal_rank(similarity_ft):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "791117a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall@1</th>\n",
       "      <th>Recall@5</th>\n",
       "      <th>MRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretrained</td>\n",
       "      <td>0.4030</td>\n",
       "      <td>0.7345</td>\n",
       "      <td>0.552257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fine-tuned</td>\n",
       "      <td>0.4705</td>\n",
       "      <td>0.8160</td>\n",
       "      <td>0.622517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model  Recall@1  Recall@5       MRR\n",
       "0  Pretrained    0.4030    0.7345  0.552257\n",
       "1  Fine-tuned    0.4705    0.8160  0.622517"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare results\n",
    "\n",
    "metrics = {\n",
    "    \"Model\": [\"Pretrained\", \"Fine-tuned\"],\n",
    "    \"Recall@1\": [\n",
    "        recall_at_k(similarity, 1),\n",
    "        recall_at_k(similarity_ft, 1)\n",
    "    ],\n",
    "    \"Recall@5\": [\n",
    "        recall_at_k(similarity, 5),\n",
    "        recall_at_k(similarity_ft, 5)\n",
    "    ],\n",
    "    \"MRR\": [\n",
    "        mean_reciprocal_rank(similarity),\n",
    "        mean_reciprocal_rank(similarity_ft)\n",
    "    ]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(metrics)\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
